{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Bert Inference benchmarks on OpneVINO with synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write benchmark commands into a file benchmark_openvino-bert.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing benchmark_openvino-bert.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile benchmark_openvino-bert.sh\n",
    "# activate opensource tf environment\n",
    "source ~/opentfov/bin/activate\n",
    "source  /opt/intel/openvino/bin/setupvars.sh\n",
    "# download OpenVINO bert IR files (.xml and .bin ) from open_model_zoo \n",
    "cd ~/oneapi_workshop\n",
    "# download FP32 files\n",
    "wget https://download.01.org/opencv/2021/openvinotoolkit/2021.1/open_model_zoo/models_bin/2/bert-large-uncased-whole-word-masking-squad-fp32-0001/FP32/bert-large-uncased-whole-word-masking-squad-fp32-0001.xml\n",
    "wget https://download.01.org/opencv/2021/openvinotoolkit/2021.1/open_model_zoo/models_bin/2/bert-large-uncased-whole-word-masking-squad-fp32-0001/FP32/bert-large-uncased-whole-word-masking-squad-fp32-0001.bin\n",
    "# Download INT8 files \n",
    "wget https://download.01.org/opencv/2021/openvinotoolkit/2021.1/open_model_zoo/models_bin/2/bert-large-uncased-whole-word-masking-squad-int8-0001/FP32-INT8/bert-large-uncased-whole-word-masking-squad-int8-0001.xml\n",
    "wget https://download.01.org/opencv/2021/openvinotoolkit/2021.1/open_model_zoo/models_bin/2/bert-large-uncased-whole-word-masking-squad-int8-0001/FP32-INT8/bert-large-uncased-whole-word-masking-squad-int8-0001.bin\n",
    "# download vocab file for words interpretation \n",
    "wget https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/models/intel/bert-large-uncased-whole-word-masking-squad-fp32-0001/vocab.txt\n",
    "\n",
    "# sample execution of benchmark app to know FPS and latency of downloaded FP32 model \n",
    "$HOME/oneapi_workshop/samples/intel64/Release/benchmark_app -m $HOME/oneapi_workshop/bert-large-uncased-whole-word-masking-squad-fp32-0001.xml -niter 100 -nstreams 1\n",
    "# sample execution of benchmark app to know FPS and latency of downloaded INT8 model \n",
    "$HOME/oneapi_workshop/samples/intel64/Release/benchmark_app -m $HOME/oneapi_workshop/bert-large-uncased-whole-word-masking-squad-int8-0001.xml -niter 100 -nstreams 1\n",
    "\n",
    "    \n",
    "deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check content of benchmark_openvino-bert.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# activate opensource tf environment\n",
      "source ~/opentfov/bin/activate\n",
      "source  /opt/intel/openvino/bin/setupvars.sh\n",
      "# download OpenVINO bert IR files (.xml and .bin ) from open_model_zoo \n",
      "cd ~/oneapi_workshop\n",
      "# download FP32 files\n",
      "wget https://download.01.org/opencv/2021/openvinotoolkit/2021.1/open_model_zoo/models_bin/2/bert-large-uncased-whole-word-masking-squad-fp32-0001/FP32/bert-large-uncased-whole-word-masking-squad-fp32-0001.xml\n",
      "wget https://download.01.org/opencv/2021/openvinotoolkit/2021.1/open_model_zoo/models_bin/2/bert-large-uncased-whole-word-masking-squad-fp32-0001/FP32/bert-large-uncased-whole-word-masking-squad-fp32-0001.bin\n",
      "# Download INT8 files \n",
      "wget https://download.01.org/opencv/2021/openvinotoolkit/2021.1/open_model_zoo/models_bin/2/bert-large-uncased-whole-word-masking-squad-int8-0001/FP32-INT8/bert-large-uncased-whole-word-masking-squad-int8-0001.xml\n",
      "wget https://download.01.org/opencv/2021/openvinotoolkit/2021.1/open_model_zoo/models_bin/2/bert-large-uncased-whole-word-masking-squad-int8-0001/FP32-INT8/bert-large-uncased-whole-word-masking-squad-int8-0001.bin\n",
      "# download vocab file for words interpretation \n",
      "wget https://raw.githubusercontent.com/openvinotoolkit/open_model_zoo/master/models/intel/bert-large-uncased-whole-word-masking-squad-fp32-0001/vocab.txt\n",
      "\n",
      "# sample execution of benchmark app to know FPS and latency of downloaded FP32 model \n",
      "$HOME/oneapi_workshop/samples/intel64/Release/benchmark_app -m $HOME/oneapi_workshop/bert-large-uncased-whole-word-masking-squad-fp32-0001.xml -niter 100 -nstreams 1\n",
      "# sample execution of benchmark app to know FPS and latency of downloaded INT8 model \n",
      "$HOME/oneapi_workshop/samples/intel64/Release/benchmark_app -m $HOME/oneapi_workshop/bert-large-uncased-whole-word-masking-squad-int8-0001.xml -niter 100 -nstreams 1\n",
      "\n",
      "    \n",
      "deactivate\n"
     ]
    }
   ],
   "source": [
    "!cat benchmark_openvino-bert.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional step : Remove all old output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf benchmark_openvino-bert.sh.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754106.v-qsvr-1.aidevcloud\n"
     ]
    }
   ],
   "source": [
    "!qsub benchmark_openvino-bert.sh -l nodes=1:clx:ppn=2 -d ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID                    Name             User            Time Use S Queue\n",
      "------------------------- ---------------- --------------- -------- - -----\n",
      "754062.v-qsvr-1            ...ub-singleuser u48334          00:00:53 R jupyterhub     \n"
     ]
    }
   ],
   "source": [
    "!qstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the job status is finished, check the the output file with proper output name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################################################\n",
      "#      Date:           Mon Dec  7 09:17:29 PST 2020\n",
      "#    Job ID:           754106.v-qsvr-1.aidevcloud\n",
      "#      User:           u48334\n",
      "# Resources:           neednodes=1:clx:ppn=2,nodes=1:clx:ppn=2,walltime=06:00:00\n",
      "########################################################################\n",
      "\n",
      "[setupvars.sh] OpenVINO environment initialized\n",
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading Inference Engine\n",
      "[ INFO ] InferenceEngine: \n",
      "\tAPI version ............ 2.1\n",
      "\tBuild .................. 2021.1.0-1237-bece22ac675-releases/2021/1\n",
      "\tDescription ....... API\n",
      "[ INFO ] Device info: \n",
      "\tCPU\n",
      "\tMKLDNNPlugin version ......... 2.1\n",
      "\tBuild ........... 2021.1.0-1237-bece22ac675-releases/2021/1\n",
      "\n",
      "[Step 3/11] Setting device configuration\n",
      "[Step 4/11] Reading network files\n",
      "[ INFO ] Loading network files\n",
      "[ INFO ] Read network took 11786.52 ms\n",
      "[Step 5/11] Resizing network to match image sizes and given batch\n",
      "[ INFO ] Network batch size: 1\n",
      "[Step 6/11] Configuring input of the model\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Load network took 4017.98 ms\n",
      "[Step 8/11] Setting optimal runtime parameters\n",
      "[Step 9/11] Creating infer requests and filling input blobs with images\n",
      "[ INFO ] Network input '0' precision I32, dimensions (NC): 1 384 \n",
      "[ INFO ] Network input '1' precision I32, dimensions (NC): 1 384 \n",
      "[ INFO ] Network input '2' precision I32, dimensions (NC): 1 384 \n",
      "[ WARNING ] No input files were given: all inputs will be filled with random values!\n",
      "[ INFO ] Infer Request 0 filling\n",
      "[ INFO ] Fill input '0' with random values (some binary data is expected)\n",
      "[ INFO ] Fill input '1' with random values (some binary data is expected)\n",
      "[ INFO ] Fill input '2' with random values (some binary data is expected)\n",
      "[Step 10/11] Measuring performance (Start inference asyncronously, 1 inference requests using 1 streams for CPU, limits: 100 iterations)\n",
      "[ INFO ] First inference took 388.17 ms\n",
      "\n",
      "[Step 11/11] Dumping statistics report\n",
      "Count:      100 iterations\n",
      "Duration:   32617.36 ms\n",
      "Latency:    321.18 ms\n",
      "Throughput: 3.07 FPS\n",
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading Inference Engine\n",
      "[ INFO ] InferenceEngine: \n",
      "\tAPI version ............ 2.1\n",
      "\tBuild .................. 2021.1.0-1237-bece22ac675-releases/2021/1\n",
      "\tDescription ....... API\n",
      "[ INFO ] Device info: \n",
      "\tCPU\n",
      "\tMKLDNNPlugin version ......... 2.1\n",
      "\tBuild ........... 2021.1.0-1237-bece22ac675-releases/2021/1\n",
      "\n",
      "[Step 3/11] Setting device configuration\n",
      "[Step 4/11] Reading network files\n",
      "[ INFO ] Loading network files\n",
      "[ INFO ] Read network took 5049.21 ms\n",
      "[Step 5/11] Resizing network to match image sizes and given batch\n",
      "[ INFO ] Network batch size: 1\n",
      "[Step 6/11] Configuring input of the model\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Load network took 21743.43 ms\n",
      "[Step 8/11] Setting optimal runtime parameters\n",
      "[Step 9/11] Creating infer requests and filling input blobs with images\n",
      "[ INFO ] Network input 'result.1' precision I32, dimensions (NC): 1 384 \n",
      "[ INFO ] Network input 'result.2' precision I32, dimensions (NC): 1 384 \n",
      "[ INFO ] Network input 'result.3' precision I32, dimensions (NC): 1 384 \n",
      "[ WARNING ] No input files were given: all inputs will be filled with random values!\n",
      "[ INFO ] Infer Request 0 filling\n",
      "[ INFO ] Fill input 'result.1' with random values (some binary data is expected)\n",
      "[ INFO ] Fill input 'result.2' with random values (some binary data is expected)\n",
      "[ INFO ] Fill input 'result.3' with random values (some binary data is expected)\n",
      "[Step 10/11] Measuring performance (Start inference asyncronously, 1 inference requests using 1 streams for CPU, limits: 100 iterations)\n",
      "[ INFO ] First inference took 236.19 ms\n",
      "\n",
      "[Step 11/11] Dumping statistics report\n",
      "Count:      100 iterations\n",
      "Duration:   16144.46 ms\n",
      "Latency:    158.51 ms\n",
      "Throughput: 6.19 FPS\n",
      "\n",
      "########################################################################\n",
      "# End of output for job 754106.v-qsvr-1.aidevcloud\n",
      "# Date: Mon Dec  7 09:21:43 PST 2020\n",
      "########################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat benchmark_openvino-bert.sh.o*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notedown the number of images/sec for training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency:    321.18 ms\n",
      "Throughput: 3.07 FPS\n",
      "Latency:    158.51 ms\n",
      "Throughput: 6.19 FPS\n"
     ]
    }
   ],
   "source": [
    "!grep  -e \"Latency\" -e \"Throughput\"  benchmark_openvino-bert.sh.o*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### close the notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
