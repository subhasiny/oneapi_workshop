{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Resnet50 Inference benchmarks on OpneVINO with synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write benchmark commands into a file benchmark_openvino-rn50.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting benchmark_openvino-rn50.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile benchmark_openvino-rn50.sh\n",
    "# activate opensource tf environment\n",
    "source ~/opentfov/bin/activate\n",
    "source  /opt/intel/openvino/bin/setupvars.sh\n",
    "\n",
    "# get trained model from intel repo\n",
    "cd ~/oneapi_workshop\n",
    "wget https://download.01.org/opencv/public_models/012020/resnet-50-tf/resnet_v1-50.pb\n",
    "#convert the tensorflow model to openVINO format\n",
    "python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --input_shape=[1,224,224,3] --mean_values=[123.68,116.78,103.94] --input=map/TensorArrayStack/TensorArrayGatherV3 --output=softmax_tensor --input_model=$HOME/oneapi_workshop/resnet_v1-50.pb --reverse_input_channels\n",
    "# run openVINO benchmarks\n",
    "# run for batch size 1 and 1 stream and notedown FPS\n",
    "$HOME/oneapi_workshop/samples/intel64/Release/benchmark_app -m $HOME/oneapi_workshop/resnet_v1-50.xml -nstreams 1 -niter 100 -b 1\n",
    "# run for batch size 1 and 4 streams and notedown FPS\n",
    "$HOME/oneapi_workshop/samples/intel64/Release/benchmark_app -m $HOME/oneapi_workshop/resnet_v1-50.xml -nstreams 4 -niter 100 -b 1\n",
    "# run for batch size 16 and 1 stream and notedown FPS\n",
    "$HOME/oneapi_workshop/samples/intel64/Release/benchmark_app -m $HOME/oneapi_workshop/resnet_v1-50.xml -nstreams 1 -niter 100 -b 16\n",
    "# run for batch size 16 and 4 streams and notedown FPS\n",
    "$HOME/oneapi_workshop/samples/intel64/Release/benchmark_app -m $HOME/oneapi_workshop/resnet_v1-50.xml -nstreams 4 -niter 100 -b 16\n",
    "deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check content of benchmark_inteltf.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# activate opensource tf environment\n",
      "source ~/opentfov/bin/activate\n",
      "source  /opt/intel/openvino/bin/setupvars.sh\n",
      "\n",
      "# get trained model from intel repo\n",
      "cd ~/oneapi_workshop\n",
      "wget https://download.01.org/opencv/public_models/012020/resnet-50-tf/resnet_v1-50.pb\n",
      "#convert the tensorflow model to openVINO format\n",
      "python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo_tf.py --input_shape=[1,224,224,3] --mean_values=[123.68,116.78,103.94] --input=map/TensorArrayStack/TensorArrayGatherV3 --output=softmax_tensor --input_model=$HOME/oneapi_workshop/resnet_v1-50.pb --reverse_input_channels\n",
      "# run openVINO benchmarks\n",
      "# run for batch size 1 and 1 stream and notedown FPS\n",
      "$HOME/oneapi_workshop/samples/intel64/Release/benchmark_app -m $HOME/oneapi_workshop/resnet_v1-50.xml -nstreams 1 -niter 100 -b 1\n",
      "# run for batch size 1 and 4 streams and notedown FPS\n",
      "$HOME/oneapi_workshop/samples/intel64/Release/benchmark_app -m $HOME/oneapi_workshop/resnet_v1-50.xml -nstreams 4 -niter 100 -b 1\n",
      "# run for batch size 16 and 1 stream and notedown FPS\n",
      "$HOME/oneapi_workshop/samples/intel64/Release/benchmark_app -m $HOME/oneapi_workshop/resnet_v1-50.xml -nstreams 1 -niter 100 -b 16\n",
      "# run for batch size 16 and 4 streams and notedown FPS\n",
      "$HOME/oneapi_workshop/samples/intel64/Release/benchmark_app -m $HOME/oneapi_workshop/resnet_v1-50.xml -nstreams 4 -niter 100 -b 16\n",
      "deactivate\n"
     ]
    }
   ],
   "source": [
    "!cat benchmark_openvino-rn50.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional step : Remove all old output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf benchmark_openvino-rn50.sh.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit to queue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "754105.v-qsvr-1.aidevcloud\n"
     ]
    }
   ],
   "source": [
    "!qsub benchmark_openvino-rn50.sh -l nodes=1:ppn=2 -d ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check job status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID                    Name             User            Time Use S Queue\n",
      "------------------------- ---------------- --------------- -------- - -----\n",
      "754062.v-qsvr-1            ...ub-singleuser u48334          00:00:51 R jupyterhub     \n"
     ]
    }
   ],
   "source": [
    "!qstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the job status is finished, check the the output file with proper output name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########################################################################\n",
      "#      Date:           Mon Dec  7 09:14:42 PST 2020\n",
      "#    Job ID:           754105.v-qsvr-1.aidevcloud\n",
      "#      User:           u48334\n",
      "# Resources:           neednodes=1:ppn=2,nodes=1:ppn=2,walltime=06:00:00\n",
      "########################################################################\n",
      "\n",
      "[setupvars.sh] OpenVINO environment initialized\n",
      "Model Optimizer arguments:\n",
      "Common parameters:\n",
      "\t- Path to the Input Model: \t/home/u48334/oneapi_workshop/resnet_v1-50.pb\n",
      "\t- Path for generated IR: \t/home/u48334/oneapi_workshop/.\n",
      "\t- IR output name: \tresnet_v1-50\n",
      "\t- Log level: \tERROR\n",
      "\t- Batch: \tNot specified, inherited from the model\n",
      "\t- Input layers: \tmap/TensorArrayStack/TensorArrayGatherV3\n",
      "\t- Output layers: \tsoftmax_tensor\n",
      "\t- Input shapes: \t[1,224,224,3]\n",
      "\t- Mean values: \t[123.68,116.78,103.94]\n",
      "\t- Scale values: \tNot specified\n",
      "\t- Scale factor: \tNot specified\n",
      "\t- Precision of IR: \tFP32\n",
      "\t- Enable fusing: \tTrue\n",
      "\t- Enable grouped convolutions fusing: \tTrue\n",
      "\t- Move mean values to preprocess section: \tNone\n",
      "\t- Reverse input channels: \tTrue\n",
      "TensorFlow specific parameters:\n",
      "\t- Input model in text protobuf format: \tFalse\n",
      "\t- Path to model dump for TensorBoard: \tNone\n",
      "\t- List of shared libraries with TensorFlow custom layers implementation: \tNone\n",
      "\t- Update the configuration file with input/output node names: \tNone\n",
      "\t- Use configuration file used to generate the model with Object Detection API: \tNone\n",
      "\t- Use the config file: \tNone\n",
      "Model Optimizer version: \t2021.1.0-1237-bece22ac675-releases/2021/1\n",
      "\n",
      "[ SUCCESS ] Generated IR version 10 model.\n",
      "[ SUCCESS ] XML file: /home/u48334/oneapi_workshop/./resnet_v1-50.xml\n",
      "[ SUCCESS ] BIN file: /home/u48334/oneapi_workshop/./resnet_v1-50.bin\n",
      "[ SUCCESS ] Total execution time: 23.12 seconds. \n",
      "[ SUCCESS ] Memory consumed: 749 MB. \n",
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading Inference Engine\n",
      "[ INFO ] InferenceEngine: \n",
      "\tAPI version ............ 2.1\n",
      "\tBuild .................. 2021.1.0-1237-bece22ac675-releases/2021/1\n",
      "\tDescription ....... API\n",
      "[ INFO ] Device info: \n",
      "\tCPU\n",
      "\tMKLDNNPlugin version ......... 2.1\n",
      "\tBuild ........... 2021.1.0-1237-bece22ac675-releases/2021/1\n",
      "\n",
      "[Step 3/11] Setting device configuration\n",
      "[Step 4/11] Reading network files\n",
      "[ INFO ] Loading network files\n",
      "[ INFO ] Read network took 946.82 ms\n",
      "[Step 5/11] Resizing network to match image sizes and given batch\n",
      "[ INFO ] Network batch size was changed to: 1\n",
      "[Step 6/11] Configuring input of the model\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Load network took 258.64 ms\n",
      "[Step 8/11] Setting optimal runtime parameters\n",
      "[Step 9/11] Creating infer requests and filling input blobs with images\n",
      "[ INFO ] Network input 'map/TensorArrayStack/TensorArrayGatherV3' precision U8, dimensions (NCHW): 1 3 224 224 \n",
      "[ WARNING ] No input files were given: all inputs will be filled with random values!\n",
      "[ INFO ] Infer Request 0 filling\n",
      "[ INFO ] Fill input 'map/TensorArrayStack/TensorArrayGatherV3' with random values (image is expected)\n",
      "[Step 10/11] Measuring performance (Start inference asyncronously, 1 inference requests using 1 streams for CPU, limits: 100 iterations)\n",
      "[ INFO ] First inference took 19.19 ms\n",
      "\n",
      "[Step 11/11] Dumping statistics report\n",
      "Count:      100 iterations\n",
      "Duration:   890.46 ms\n",
      "Latency:    8.70 ms\n",
      "Throughput: 112.30 FPS\n",
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading Inference Engine\n",
      "[ INFO ] InferenceEngine: \n",
      "\tAPI version ............ 2.1\n",
      "\tBuild .................. 2021.1.0-1237-bece22ac675-releases/2021/1\n",
      "\tDescription ....... API\n",
      "[ INFO ] Device info: \n",
      "\tCPU\n",
      "\tMKLDNNPlugin version ......... 2.1\n",
      "\tBuild ........... 2021.1.0-1237-bece22ac675-releases/2021/1\n",
      "\n",
      "[Step 3/11] Setting device configuration\n",
      "[Step 4/11] Reading network files\n",
      "[ INFO ] Loading network files\n",
      "[ INFO ] Read network took 904.63 ms\n",
      "[Step 5/11] Resizing network to match image sizes and given batch\n",
      "[ INFO ] Network batch size was changed to: 1\n",
      "[Step 6/11] Configuring input of the model\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Load network took 543.82 ms\n",
      "[Step 8/11] Setting optimal runtime parameters\n",
      "[Step 9/11] Creating infer requests and filling input blobs with images\n",
      "[ INFO ] Network input 'map/TensorArrayStack/TensorArrayGatherV3' precision U8, dimensions (NCHW): 1 3 224 224 \n",
      "[ WARNING ] No input files were given: all inputs will be filled with random values!\n",
      "[ INFO ] Infer Request 0 filling\n",
      "[ INFO ] Fill input 'map/TensorArrayStack/TensorArrayGatherV3' with random values (image is expected)\n",
      "[ INFO ] Infer Request 1 filling\n",
      "[ INFO ] Fill input 'map/TensorArrayStack/TensorArrayGatherV3' with random values (image is expected)\n",
      "[ INFO ] Infer Request 2 filling\n",
      "[ INFO ] Fill input 'map/TensorArrayStack/TensorArrayGatherV3' with random values (image is expected)\n",
      "[ INFO ] Infer Request 3 filling\n",
      "[ INFO ] Fill input 'map/TensorArrayStack/TensorArrayGatherV3' with random values (image is expected)\n",
      "[Step 10/11] Measuring performance (Start inference asyncronously, 4 inference requests using 4 streams for CPU, limits: 100 iterations)\n",
      "[ INFO ] First inference took 37.85 ms\n",
      "\n",
      "[Step 11/11] Dumping statistics report\n",
      "Count:      100 iterations\n",
      "Duration:   559.90 ms\n",
      "Latency:    21.31 ms\n",
      "Throughput: 178.60 FPS\n",
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading Inference Engine\n",
      "[ INFO ] InferenceEngine: \n",
      "\tAPI version ............ 2.1\n",
      "\tBuild .................. 2021.1.0-1237-bece22ac675-releases/2021/1\n",
      "\tDescription ....... API\n",
      "[ INFO ] Device info: \n",
      "\tCPU\n",
      "\tMKLDNNPlugin version ......... 2.1\n",
      "\tBuild ........... 2021.1.0-1237-bece22ac675-releases/2021/1\n",
      "\n",
      "[Step 3/11] Setting device configuration\n",
      "[Step 4/11] Reading network files\n",
      "[ INFO ] Loading network files\n",
      "[ INFO ] Read network took 94.50 ms\n",
      "[Step 5/11] Resizing network to match image sizes and given batch\n",
      "[ INFO ] Reshaping network: 'map/TensorArrayStack/TensorArrayGatherV3': [16, 3, 224, 224]\n",
      "[ INFO ] Reshape network took 2.98 ms\n",
      "[ INFO ] Network batch size was changed to: 16\n",
      "[Step 6/11] Configuring input of the model\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Load network took 241.06 ms\n",
      "[Step 8/11] Setting optimal runtime parameters\n",
      "[Step 9/11] Creating infer requests and filling input blobs with images\n",
      "[ INFO ] Network input 'map/TensorArrayStack/TensorArrayGatherV3' precision U8, dimensions (NCHW): 16 3 224 224 \n",
      "[ WARNING ] No input files were given: all inputs will be filled with random values!\n",
      "[ INFO ] Infer Request 0 filling\n",
      "[ INFO ] Fill input 'map/TensorArrayStack/TensorArrayGatherV3' with random values (image is expected)\n",
      "[Step 10/11] Measuring performance (Start inference asyncronously, 1 inference requests using 1 streams for CPU, limits: 100 iterations)\n",
      "[ INFO ] First inference took 136.23 ms\n",
      "\n",
      "[Step 11/11] Dumping statistics report\n",
      "Count:      100 iterations\n",
      "Duration:   8777.85 ms\n",
      "Latency:    86.52 ms\n",
      "Throughput: 182.28 FPS\n",
      "[Step 1/11] Parsing and validating input arguments\n",
      "[ INFO ] Parsing input parameters\n",
      "[Step 2/11] Loading Inference Engine\n",
      "[ INFO ] InferenceEngine: \n",
      "\tAPI version ............ 2.1\n",
      "\tBuild .................. 2021.1.0-1237-bece22ac675-releases/2021/1\n",
      "\tDescription ....... API\n",
      "[ INFO ] Device info: \n",
      "\tCPU\n",
      "\tMKLDNNPlugin version ......... 2.1\n",
      "\tBuild ........... 2021.1.0-1237-bece22ac675-releases/2021/1\n",
      "\n",
      "[Step 3/11] Setting device configuration\n",
      "[Step 4/11] Reading network files\n",
      "[ INFO ] Loading network files\n",
      "[ INFO ] Read network took 81.64 ms\n",
      "[Step 5/11] Resizing network to match image sizes and given batch\n",
      "[ INFO ] Reshaping network: 'map/TensorArrayStack/TensorArrayGatherV3': [16, 3, 224, 224]\n",
      "[ INFO ] Reshape network took 2.89 ms\n",
      "[ INFO ] Network batch size was changed to: 16\n",
      "[Step 6/11] Configuring input of the model\n",
      "[Step 7/11] Loading the model to the device\n",
      "[ INFO ] Load network took 522.62 ms\n",
      "[Step 8/11] Setting optimal runtime parameters\n",
      "[Step 9/11] Creating infer requests and filling input blobs with images\n",
      "[ INFO ] Network input 'map/TensorArrayStack/TensorArrayGatherV3' precision U8, dimensions (NCHW): 16 3 224 224 \n",
      "[ WARNING ] No input files were given: all inputs will be filled with random values!\n",
      "[ INFO ] Infer Request 0 filling\n",
      "[ INFO ] Fill input 'map/TensorArrayStack/TensorArrayGatherV3' with random values (image is expected)\n",
      "[ INFO ] Infer Request 1 filling\n",
      "[ INFO ] Fill input 'map/TensorArrayStack/TensorArrayGatherV3' with random values (image is expected)\n",
      "[ INFO ] Infer Request 2 filling\n",
      "[ INFO ] Fill input 'map/TensorArrayStack/TensorArrayGatherV3' with random values (image is expected)\n",
      "[ INFO ] Infer Request 3 filling\n",
      "[ INFO ] Fill input 'map/TensorArrayStack/TensorArrayGatherV3' with random values (image is expected)\n",
      "[Step 10/11] Measuring performance (Start inference asyncronously, 4 inference requests using 4 streams for CPU, limits: 100 iterations)\n",
      "[ INFO ] First inference took 330.26 ms\n",
      "\n",
      "[Step 11/11] Dumping statistics report\n",
      "Count:      100 iterations\n",
      "Duration:   8048.56 ms\n",
      "Latency:    313.74 ms\n",
      "Throughput: 198.79 FPS\n",
      "\n",
      "########################################################################\n",
      "# End of output for job 754105.v-qsvr-1.aidevcloud\n",
      "# Date: Mon Dec  7 09:15:43 PST 2020\n",
      "########################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!cat benchmark_openvino-rn50.sh.o*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notedown the number of images/sec for training and inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ INFO ] Network batch size was changed to: 1\n",
      "[Step 10/11] Measuring performance (Start inference asyncronously, 1 inference requests using 1 streams for CPU, limits: 100 iterations)\n",
      "Throughput: 112.30 FPS\n",
      "[ INFO ] Network batch size was changed to: 1\n",
      "[Step 10/11] Measuring performance (Start inference asyncronously, 4 inference requests using 4 streams for CPU, limits: 100 iterations)\n",
      "Throughput: 178.60 FPS\n",
      "[ INFO ] Network batch size was changed to: 16\n",
      "[Step 10/11] Measuring performance (Start inference asyncronously, 1 inference requests using 1 streams for CPU, limits: 100 iterations)\n",
      "Throughput: 182.28 FPS\n",
      "[ INFO ] Network batch size was changed to: 16\n",
      "[Step 10/11] Measuring performance (Start inference asyncronously, 4 inference requests using 4 streams for CPU, limits: 100 iterations)\n",
      "Throughput: 198.79 FPS\n"
     ]
    }
   ],
   "source": [
    "!grep -e \"batch size\" -e \"inference requests\" -e \"Throughput\"  benchmark_openvino-rn50.sh.o*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### close the notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (Intel® oneAPI)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_oneapi-beta05-python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
